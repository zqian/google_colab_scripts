{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rubric Item Report",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgC8PGt2aOYQja9eHDIIbe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zqian/google_colab_scripts/blob/main/Rubric_Item_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this [Canvas Idea Conversation](https://community.canvaslms.com/t5/Idea-Conversations/Export-aggregate-rubric-scores-to-identify-strengths-and/idi-p/370265/page/3?emcs_t=S2h8ZW1haWx8ZGlnZXN0X25vdGlmaWNhdGlvbnxLWU9HSEw3TFIzVVpFOHwtMXxPVEhFUlN8aEs#comments), here is a PoC script to generate a csv file that shows the rubric line item scores for all student submissions."
      ],
      "metadata": {
        "id": "I69qFOzw4b64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Python Libraries**"
      ],
      "metadata": {
        "id": "TkrZ5-d97xQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbRAaNHr4U-k"
      },
      "outputs": [],
      "source": [
        "# install all libraries\n",
        "!pip install canvasapi\n",
        "!pip install future-fstrings\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: get Canvas credentials**\n",
        "\n",
        "*   Log into Canvas instance\n",
        "*   Go to \"Account\"-> \"Settings\" -> \"+ New Access Token\". You can name it as \"Rubric Ttem Report\", and leave the \"Expires\" field empty for no expiration. Click on \"Generate Token\"\n",
        "*   Copy the token into next section"
      ],
      "metadata": {
        "id": "JI46VV7R8FTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from canvasapi import Canvas\n",
        "from canvasapi.assignment import (\n",
        "    Assignment,\n",
        "    AssignmentExtension,\n",
        "    AssignmentGroup,\n",
        "    AssignmentOverride,\n",
        ")\n",
        "\n",
        "from canvasapi.rubric import Rubric, RubricAssociation\n",
        "from canvasapi.submission import GroupedSubmission, Submission\n",
        "import pandas as pd\n",
        "\n",
        "PERSONAL_ACCESS_TOKEN = 'replace_with_assess_token'\n",
        "CANVAS_URL = 'replace_with_institution_canvas_url'\n",
        "COURSE_ID = 'replace_with_canvas_course_id'\n",
        "ASSIGNMENT_ID = 'replace_with_canvas_assignment_id'"
      ],
      "metadata": {
        "id": "z1OKTnNU9LNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Retrieve assignment and its rubric settings**"
      ],
      "metadata": {
        "id": "h9AeXSHAE3i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init CanvasAPI library\n",
        "# get course, assignment with rubric settings\n",
        "canvas = Canvas(CANVAS_URL, PERSONAL_ACCESS_TOKEN)\n",
        "course = canvas.get_course(COURSE_ID)\n",
        "\n",
        "assignment = course.get_assignment(ASSIGNMENT_ID, include=['overrides'])\n",
        "assignment_name = assignment.name\n",
        "\n",
        "# get assignment rubric settings object; retrieve the rubric id\n",
        "assignment_rubric_settings = assignment.rubric_settings\n",
        "assignment_rubric_id = assignment_rubric_settings['id']\n",
        "assignment_rubric_title = assignment_rubric_settings['title']\n",
        "assignment_rubric_points_possible = assignment_rubric_settings['points_possible']\n",
        "\n",
        "# get rubric settings\n",
        "rubric_settings = assignment.rubric\n",
        "for setting in rubric_settings:\n",
        "  print(f\"{setting['id']} {setting['description']}\")"
      ],
      "metadata": {
        "id": "oo0jz7CKJkfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: iterating through assignment submissions. Output csv report file for each rubric criteria**"
      ],
      "metadata": {
        "id": "x5CJoNWXFCSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "submissions = assignment.get_submissions(include=['rubric_assessment'])\n",
        "\n",
        "for setting in rubric_settings:\n",
        "  print(f\"{setting['id']} {setting['description']}\")\n",
        "  rubric_item_id = setting['id']\n",
        "  rubric_item_description = setting['description']\n",
        "\n",
        "  # the output csv file columns\n",
        "  # you can modify the column to include other attributes, e.g. student name, points_possible, etc\n",
        "  df = pd.DataFrame(columns = [\"user_id\", \"points\", \"comments\"])\n",
        "  for submission in submissions:\n",
        "    try:\n",
        "      if submission.rubric_assessment:\n",
        "        df = df.append({\"user_id\": submission.user_id,\n",
        "                  \"points\": submission.rubric_assessment[rubric_item_id]['points'],\n",
        "                  \"comments\": submission.rubric_assessment[rubric_item_id]['comments']}, ignore_index=True)\n",
        "    except:\n",
        "      print(f\"no rubric assessment for submission {submission}\")\n",
        "  \n",
        "  # the output file is named after \"{assignment_name}_{rubric_item_description}_report.csv\"\n",
        "  # you can change the file name format acoordingly\n",
        "  download_file_name = f\"{assignment_name}_{rubric_item_description}_report.csv\"\n",
        "  df.to_csv(download_file_name, index=False)\n",
        "  files.download(download_file_name) \n",
        "    "
      ],
      "metadata": {
        "id": "ICpSiLjKAmre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xSXRXTov4Yzh"
      }
    }
  ]
}